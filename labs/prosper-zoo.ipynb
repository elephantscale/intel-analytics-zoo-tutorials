{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network : Prosper Loan Dataset\n",
    "\n",
    "We are going to look at the prosper loan dataset.  This dataset shows a history of loans made by Prosper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## small file, start with this\n",
    "#datafile = \"https://s3.amazonaws.com/elephantscale-public/data/prosper-loan/prosper-loan-data-sample.csv\"\n",
    "## this is a large file\n",
    "datafile = \"https://s3.amazonaws.com/elephantscale-public/data/prosper-loan/prosper-loan-data.csv.gz\"\n",
    "\n",
    "data = pd.read_csv(datafile)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO : select a few columns \n",
    "## start with: 'LoanStatus',  'EmploymentStatus', 'CreditScore', 'StatedMonthlyIncome', 'ListingCategory'\n",
    "#select_columns = ['LoanStatus', 'EmploymentStatus', 'CreditScore', '???', '???']\n",
    "\n",
    "\n",
    "## we can add more later\n",
    "\n",
    "select_columns = ['LoanStatus',  'EmploymentStatus', 'CreditScore', 'StatedMonthlyIncome', 'ListingCategory']\n",
    "\n",
    "## Note : vector columns can only have Numbers, don't include Categorical columns here\n",
    "## And definitely not 'LoanStatus'  (if you are curiuos include and see what happens!)\n",
    "vector_columns = [ 'EmpIndex', 'CreditScore', 'StatedMonthlyIncome', 'CategoryIndex']\n",
    "\n",
    "## Feature Columns\n",
    "\n",
    "feature_columns = ['EmploymentStatusFactor', 'CreditScore', 'StatedMonthlyIncome', 'ListingCategoryFactor']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO :  Drop any NA, null values.  \n",
    "## Hint : Using `.na.drop()`\n",
    "prosper_clean = data.dropna()\n",
    "\n",
    "print(\"Original record count {:,}, cleaned records count {:,},  dropped {:,}\"\\\n",
    "      .format(len(data), len(prosper_clean), \n",
    "              (len(data) - len(prosper_clean))))\n",
    "prosper_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some summary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prosper_clean['LoanStatus'].value_counts())\n",
    "print(prosper_clean['EmploymentStatus'].value_counts())\n",
    "print(prosper_clean['ListingCategory'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> What does that say about the cardinality of these categorical columns? ***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Converting Categorical columns \n",
    "\n",
    "Convert categorical columns to numeric.   \n",
    "Here let's convert **EmploymentStatus** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use pd.factorize on EmploymentStatus, ListingCategory\n",
    "\n",
    "prosper_clean['EmploymentStatusFactor'] = pd.factorize(prosper_clean['EmploymentStatus'])[0]\n",
    "prosper_clean['ListingCategoryFactor'] = pd.factorize(prosper_clean['ListingCategory'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build feature vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = prosper_clean[feature_columns]\n",
    "label = prosper_clean['LoanStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Split Data into training and test.\n",
    "\n",
    "We will split our the data up into training and test.  (You know the drill by now).\n",
    "\n",
    "**=> TODO: Split dataset into 70% training, 30% validation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO :  Split the data into 70% training and 30% test sets \n",
    "## Hint : 0.7   , 0.3\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, label)\n",
    "print(\"training set = \" , len(train_x))\n",
    "print(\"testing set = \" , len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(train_y)\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train_x):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu, input_dim=4),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "\n",
    "\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "\n",
    "model = build_model(train_x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "model.fit(\n",
    "  train_x, train_y,\n",
    "  epochs=EPOCHS, validation_split = 0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the model.\n",
    "\n",
    "Let us check to see how the model did, using accuracy as a measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 8: Improve Accuracy\n",
    "\n",
    "### Add more features\n",
    "Look at the schema of the full dataset.  Are there any columns you want to add. Make sure you up the number of neurons in the hidden layer as you add more features."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
